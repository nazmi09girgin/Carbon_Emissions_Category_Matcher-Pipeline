{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMPMqNDggOM4"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NAICS Eşleştirme Pipeline (Embedding + Cosine Similarity + LLM Hakemliği)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Bu betik, muhasebe/fatura verilerini NAICS sınıfları ile eşleştirmek üzere\n",
    "# üç aşamalı bir yaklaşım uygular:\n",
    "#   1) Veri hazırlama ve metin temizliği\n",
    "#   2) Çok dilli cümle gömme (Sentence-Transformers) + cosine similarity ile\n",
    "#      aday sınıfların oluşturulması (Top-K)\n",
    "#   3) LLM ile nihai seçim (hakemlik) ve açıklama üretimi\n",
    "#\n",
    "# Açıklama:\n",
    "# - Embedding: Serbest metinleri (tedarikçi adı + ürün açıklamaları) anlamsal\n",
    "#   uzayda sayısal vektörlere çevirir. Farklı diller/kısaltmalar/biçimler\n",
    "#   arasındaki anlamsal benzerlikleri yakalamak için idealdir.\n",
    "# - Cosine Similarity: İki vektörün yön benzerliğini ölçer [–1, +1]. 1’e\n",
    "#   yakın skorlar yüksek anlamsal örtüşme demektir. Eşik (THRESHOLD) ile\n",
    "#   \"yeterince benzer\" adayları işaretleriz.\n",
    "# - LLM Hakemliği: Benzer adaylar arasında bağlama (ürün kalemlerinin özü,\n",
    "#   jenerik ücretler vs.) dikkat ederek tek bir NAICS seçer ve kısa bir\n",
    "#   gerekçe üretir. Embedding’in kaçırabileceği iş bağlamı sinyallerini\n",
    "#   toparlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IOnArha6EhwE"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers scikit-learn pandas openpyxl transformers torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQIShcDPEwpH"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) KÜTÜPHANELER\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vZ3zafDfEbG-"
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 2) MUHASEBE VERİSİ HAZIRLAMA\n",
    "# ============================\n",
    "# Excel dosyasını okuyup, fatura bazlı (Fiş/Fatura No + Tedarikçi) gruplama yapar ve ürün/hizmet kalemlerini aynı satırda virgülle birleştirir.\n",
    "def gruplu_veri_olustur(dosya_yolu):\n",
    "    print(f\"'{dosya_yolu}' okunuyor...\")\n",
    "    df = pd.read_excel(dosya_yolu)\n",
    "    df['Fiş/Fatura No'] = df['Fiş/Fatura No'].ffill()\n",
    "    df['Tedarikçi / Çalışan'] = df['Tedarikçi / Çalışan'].ffill()\n",
    "    print(\"Veri fatura bazında gruplanıyor...\")\n",
    "    df_grouped = df.groupby(['Fiş/Fatura No', 'Tedarikçi / Çalışan'])['Ürün/hizmet'] \\\n",
    "                   .apply(lambda x: ', '.join(x.dropna().astype(str))) \\\n",
    "                   .reset_index()\n",
    "    df_grouped.columns = ['Fatura_No', 'Tedarikçi_Orjinal', 'Urunler_Orjinal']\n",
    "    print(\"Gruplama tamamlandı.\")\n",
    "    return df_grouped\n",
    "\n",
    "#TEMİZLEME FONKSİYONU\n",
    "def clean_text_ultimate_pro(text): # Tedarikçi ve ürün/hizmet metinlerini normalleştirir ve gürültüyü temizler.\n",
    "    text = str(text)\n",
    "    text = text.replace('İ', 'i').lower()\n",
    "\n",
    "    text = re.sub(r'\\ba\\.(s|ş)\\b', '', text, flags=re.IGNORECASE) #aş as gibi ikili kısaltma için\n",
    "\n",
    "    # 1. Noktaları boşlukla değiştir\n",
    "    text = text.replace('.', ' ')\n",
    "\n",
    "    # 2. Fazla boşlukları temizle\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 3. Türkçe karakterleri sadeleştir\n",
    "    turkish_chars = {'ç': 'c', 'ğ': 'g', 'ı': 'i', 'ö': 'o', 'ş': 's', 'ü': 'u'}\n",
    "    for tr, en in turkish_chars.items():\n",
    "        text = text.replace(tr, en)\n",
    "\n",
    "    # 4. Yaygın kısaltmaları kaldır\n",
    "    abbreviations = [\n",
    "        'as', 'ltd', 'sti', 'tic', 'tur', 'tas', 'mal', 'ins', 'amb', 've', 'san', 'limited', 'sirketi','anonim', 'sirket'\n",
    "    ]\n",
    "    pattern_abbr = r'\\b(' + '|'.join(abbreviations) + r')\\b'\n",
    "    text = re.sub(pattern_abbr, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 5. Ölçü birimlerini sil\n",
    "    units = ['kg', 'ml', 'lt','l', 'gr', 'cl', 'mm', 'cm', 'x']\n",
    "    pattern_unit = r'\\d+\\s*(' + '|'.join(units) + r')\\b'\n",
    "    text = re.sub(pattern_unit, '', text)\n",
    "\n",
    "    # 6. Sayı, özel karakter ve son temizlik\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^,\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\s*,\\s*', ',', text)\n",
    "    text = re.sub(r',,+', ',', text).strip(',')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- ANA İŞ AKIŞI ---\n",
    "try:\n",
    "    dosya_yolu = \"/content/yeni_girdi_muhasebe.xlsx\"\n",
    "    df_final = gruplu_veri_olustur(dosya_yolu)\n",
    "\n",
    "    print(\"Tedarikçi ve Ürünler sütunları en gelişmiş fonksiyon ile temizleniyor...\")\n",
    "    df_final['Tedarikçi_Temiz'] = df_final['Tedarikçi_Orjinal'].apply(clean_text_ultimate_pro)\n",
    "    df_final['Urunler_Temiz'] = df_final['Urunler_Orjinal'].apply(clean_text_ultimate_pro)\n",
    "\n",
    "    df_final['Model_Girdisi'] = df_final['Tedarikçi_Temiz'] + ', ' + df_final['Urunler_Temiz']\n",
    "    df_final['Model_Girdisi'] = df_final['Model_Girdisi'].apply(lambda x: ', '.join(dict.fromkeys([i.strip() for i in re.sub(r',,+', ',', x).strip(',').split(',')])))\n",
    "\n",
    "    print(\"\\nİşlem Tamamlandı! Sonucun ilk 5 satırı:\")\n",
    "\n",
    "    display_cols = ['Tedarikçi_Orjinal', 'Urunler_Orjinal', 'Urunler_Temiz', 'Model_Girdisi']\n",
    "    print(df_final[display_cols].head().to_string())\n",
    "\n",
    "    output_filename = \"Temizlenmis_v2.xlsx\"\n",
    "    df_final.to_excel(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nVeri başarıyla temizlendi ve '{output_filename}' adıyla kaydedildi.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n!!! HATA: '{dosya_yolu}' dosyası bulunamadı.\")\n",
    "    print(\"Lütfen sol taraftaki 'Dosyalar' bölümüne dosyayı yüklediğinizden emin olun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "z2FDsNc5Eeuf"
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3) NAICS VERİSİ HAZIRLAMA\n",
    "# ============================\n",
    "\n",
    "# NAICS metinlerini temizlemek için fonksiyon\n",
    "def clean_text_for_naics(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^,\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\s*,\\s*', ',', text)\n",
    "    text = re.sub(r',,+', ',', text).strip(',')\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_phrases(text):\n",
    "    parts = [p.strip() for p in text.split(',')]\n",
    "    unique = list(dict.fromkeys(parts))  # Sıralı ve tekrarsız\n",
    "    return ', '.join(unique)\n",
    "\n",
    "try:\n",
    "    # NAICS dosyasını oku\n",
    "    naics_df = pd.read_excel(\"NAICS Codes & Descriptions.xlsx\")\n",
    "    print(\"NAICS Codes & Descriptions.xlsx başarıyla yüklendi.\")\n",
    "\n",
    "    # Boş satır olmadığından emin olmak için temel bir 'dropna' yeterli\n",
    "    naics_df = naics_df.dropna(subset=['NAICS Codes', '2017 NAICS Titles (USA)', 'Index Item Descriptions'])\n",
    "\n",
    "    # NAICS koduna göre grupla ve tüm açıklamaları birleştir\n",
    "    print(\"NAICS kodları gruplanıyor ve açıklamalar birleştiriliyor...\")\n",
    "    naics_grouped = naics_df.groupby('NAICS Codes').agg(\n",
    "        NAICS_Title=('2017 NAICS Titles (USA)', 'first'),\n",
    "        NAICS_Item_Descriptions=('Index Item Descriptions', lambda x: ', '.join(x.astype(str)))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Modele verilecek nihai birleşik açıklamayı oluştur\n",
    "    naics_grouped['Combined_Description'] = naics_grouped['NAICS_Title'] + ', ' + naics_grouped['NAICS_Item_Descriptions']\n",
    "\n",
    "    # Bu birleşik açıklamayı temizle\n",
    "    print(\"Birleştirilmiş NAICS açıklamaları temizleniyor...\")\n",
    "    naics_grouped['Cleaned_Description'] = naics_grouped['Combined_Description'].apply(clean_text_for_naics)\n",
    "    naics_grouped['Cleaned_Description'] = naics_grouped['Cleaned_Description'].apply(remove_duplicate_phrases)\n",
    "\n",
    "    print(\"\\nGruplanmış ve Temizlenmiş NAICS Verisi Örneği:\")\n",
    "    print(naics_grouped[['NAICS Codes', 'Cleaned_Description']].head().to_string())\n",
    "\n",
    "    output_filename = \"2_Adim_Hazir_NAICS_Verisi.xlsx\"\n",
    "    naics_grouped.to_excel(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nNAICS verisi başarıyla hazırlandı ve '{output_filename}' adıyla kaydedildi.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n!!! HATA: 'NAICS Codes & Descriptions.xlsx' dosyası bulunamadı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NCkoeUpzCLZ-"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) EŞLEŞTİRME (Embedding + LLM Hakemliği - meta-llama/Llama-3.1-8B-Instruct)\n",
    "# =========================\n",
    "\n",
    "# Parametreler\n",
    "MUHASEBE_PATH = \"/content/Temizlenmis_v2.xlsx\"\n",
    "NAICS_PATH = \"/content/2_Adim_Hazir_NAICS_Verisi.xlsx\"\n",
    "OUT_PATH = \"/content/Yeni_Final_Eslesme_LLM.xlsx\"\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" # çok dilli, güçlü semantik gömme\n",
    "LLM_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\" # çok dilli llm modeli\n",
    "\n",
    "HF_TOKEN = \"\"  #'hf_xxx' şeklinde Hugging Face token buraya koy\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "\n",
    "TOP_K = 10 # LLM'e göstereceğimiz aday sayısı (Top-10)\n",
    "TOP3_K = 3 # Rapor için\n",
    "THRESHOLD = 0.50\n",
    "\n",
    "# Embedding modeli\n",
    "print(\"Embedding modeli yükleniyor...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "# Dosyaları yükle\n",
    "df_muhasebe = pd.read_excel(MUHASEBE_PATH)\n",
    "df_naics = pd.read_excel(NAICS_PATH)\n",
    "\n",
    "# =========================\n",
    "# 5) EMBEDDING VE BENZERLİK COSINE SIMILARITY\n",
    "# =========================\n",
    "\n",
    "# Cosine similaritY\n",
    "# - Normdan bağımsız yönsel benzerlik ölçer. Vektör uzunluğundan etkilenmez.\n",
    "# - 0.50 üzeri skorlar (pratikte) çoğu veri kümesinde anlamlı benzerlik sinyali verir.\n",
    "\n",
    "print(\" Embedding vektörleri oluşturuluyor...\")\n",
    "muhasebe_embeddings = embed_model.encode(df_muhasebe[\"Model_Girdisi\"].astype(str).tolist(), show_progress_bar=True)\n",
    "naics_embeddings = embed_model.encode(df_naics[\"Cleaned_Description\"].astype(str).tolist(), show_progress_bar=True)\n",
    "\n",
    "results = []\n",
    "print(\" Eşleştirme işlemi başlatıldı...\")\n",
    "print(\" Eşleştirme işlemi başlatıldı...\")\n",
    "for i, query_vec in enumerate(tqdm(muhasebe_embeddings)):\n",
    "    scores = util.cos_sim(query_vec, naics_embeddings)[0]  # (N,)\n",
    "    n = scores.shape[0]\n",
    "\n",
    "    # Top-10 shortlist\n",
    "    k10 = min(TOP_K, n)\n",
    "    top10 = torch.topk(scores, k=k10)\n",
    "    top10_idx = top10.indices.tolist()\n",
    "    top10_kodlar = [str(df_naics.loc[j, \"NAICS Codes\"]) for j in top10_idx]\n",
    "    top10_kategoriler = [str(df_naics.loc[j, \"NAICS_Title\"]) for j in top10_idx]\n",
    "\n",
    "    # Top-3 (rapor için)\n",
    "    k3 = min(TOP3_K, n)\n",
    "    top3 = torch.topk(scores, k=k3)\n",
    "    top3_kodlar = [str(df_naics.loc[j, \"NAICS Codes\"]) for j in top3.indices.tolist()]\n",
    "    top3_kategoriler = [str(df_naics.loc[j, \"NAICS_Title\"]) for j in top3.indices.tolist()]\n",
    "\n",
    "    # Embedding tabanlı en iyi aday ve skor\n",
    "    best_score = top10.values[0].item()\n",
    "    best_idx = top10.indices[0].item()\n",
    "\n",
    "    # Eşik mantığı:\n",
    "    # - Skor >= THRESHOLD ise \"embedding top-1 yeterince güçlü\" kabul edilir.\n",
    "    # - Aksi halde LLM nihai seçim yapacaktır\n",
    "    if best_score >= THRESHOLD:\n",
    "        matched_code = df_naics.loc[best_idx, \"NAICS Codes\"]\n",
    "        matched_title = df_naics.loc[best_idx, \"NAICS_Title\"]\n",
    "    else:\n",
    "        matched_code = \"Kontrol Et\"\n",
    "        matched_title = \"Kontrol Et\"\n",
    "\n",
    "    results.append({\n",
    "        \"Fatura_No\": df_muhasebe.loc[i, \"Fatura_No\"],\n",
    "        \"Tedarikçi_Orjinal\": df_muhasebe.loc[i, \"Tedarikçi_Orjinal\"],\n",
    "        \"Urunler_Orjinal\": df_muhasebe.loc[i, \"Urunler_Orjinal\"],\n",
    "        \"Atanan_NAICS_Kodu\": matched_code,\n",
    "        \"Eşleşen_NAICS_Kategorisi\": matched_title,\n",
    "        \"Benzerlik_Skoru\": round(best_score, 4),\n",
    "        \"Top_3_Kodlar\": \", \".join(top3_kodlar),\n",
    "        \"Top_3_Kategoriler\": \"; \".join(top3_kategoriler),\n",
    "        \"Top_10_Kodlar\": \", \".join(top10_kodlar),\n",
    "        \"Top_10_Kategoriler\": \"; \".join(top10_kategoriler),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# =========================\n",
    "# 6) LLM MODELİ YÜKLEME\n",
    "# =========================\n",
    "# LLM, embedding ile belirlediğimiz adaylar arasından bağlama en uygun TEK sınıfı seçer.\n",
    "# 4-bit quantization bellek kazanımı ve hız için\n",
    "print(f\"\\n LLM modeli yükleniyor: {LLM_MODEL_ID}\")\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, token=HF_TOKEN)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_ID, token=HF_TOKEN, device_map=\"auto\", quantization_config=quant_config\n",
    ")\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "\n",
    "def llm_generate_json(messages):\n",
    "    # Chat modellerinde en doğrusu: apply_chat_template\n",
    "    prompt_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    out = llm_pipe(\n",
    "        prompt_str,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.01,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    return out[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) LLM İLE NİHAİ KARAR (HAKEMLİK)\n",
    "# =========================\n",
    "def build_llm_prompt(row):\n",
    "    codes = [c.strip() for c in str(row[\"Top_10_Kodlar\"]).split(\",\") if c.strip()]\n",
    "    cats  = [c.strip() for c in str(row[\"Top_10_Kategoriler\"]).split(\";\") if c.strip()]\n",
    "\n",
    "    options = []\n",
    "    for i in range(len(codes)):\n",
    "        cat = cats[i] if i < len(cats) else \"\"\n",
    "        options.append(f\"[{i+1}] {codes[i]} — {cat}\")\n",
    "    options_str = \"\\n\".join(options)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a meticulous accounting analyst from Turkey. \"\n",
    "        \"Your task is to analyze an invoice's line items and choose the SINGLE most appropriate NAICS category from a given shortlist. \"\n",
    "        \"Focus on the core service, ignoring generic fees. \"\n",
    "        \"You MUST respond ONLY with a valid JSON object containing 'selection_index' (integer 1-10) and 'reasoning' (string, max 2 sentences in Turkish).\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"FATURA BİLGİLERİ:\n",
    "- Fatura No: {row['Fatura_No']}\n",
    "- Tedarikçi: {row['Tedarikçi_Orjinal']}\n",
    "- Kalemler: {row['Urunler_Orjinal']}\n",
    "\n",
    "NAICS KATEGORİ ADAYLARI:\n",
    "{options_str}\n",
    "\n",
    "Cevabını sadece istenen JSON formatında ver.\n",
    "\"\"\"\n",
    "\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "def try_parse_json(text):\n",
    "    import json, re\n",
    "    try:\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
    "        return json.loads(m.group(0)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "llm_codes, llm_titles, llm_reasonings = [], [], []\n",
    "\n",
    "print(\"\\n LLM ile nihai seçimler yapılıyor...\")\n",
    "for _, row in df_results.iterrows():\n",
    "    try:\n",
    "        messages = build_llm_prompt(row)\n",
    "        raw_text = llm_generate_json(messages)\n",
    "        parsed = try_parse_json(raw_text)\n",
    "\n",
    "        if parsed and \"selection_index\" in parsed:\n",
    "            sel_idx = int(parsed[\"selection_index\"]) - 1\n",
    "            codes = [c.strip() for c in str(row[\"Top_10_Kodlar\"]).split(\",\") if c.strip()]\n",
    "            cats  = [c.strip() for c in str(row[\"Top_10_Kategoriler\"]).split(\";\") if c.strip()]\n",
    "            if 0 <= sel_idx < len(codes):\n",
    "                llm_codes.append(codes[sel_idx])\n",
    "                llm_titles.append(cats[sel_idx] if sel_idx < len(cats) else \"\")\n",
    "                llm_reasonings.append(parsed.get(\"reasoning\", \"\"))\n",
    "                continue\n",
    "\n",
    "        # Parse başarısızsa/indeks geçersizse: embedding top-1’e fallback\n",
    "        llm_codes.append(row[\"Atanan_NAICS_Kodu\"])\n",
    "        llm_titles.append(row[\"Eşleşen_NAICS_Kategorisi\"])\n",
    "        llm_reasonings.append(\"LLM parse/index sorunlu, embedding top-1 ile devam.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        llm_codes.append(row[\"Atanan_NAICS_Kodu\"])\n",
    "        llm_titles.append(row[\"Eşleşen_NAICS_Kategorisi\"])\n",
    "        llm_reasonings.append(f\"LLM error: {str(e)}\")\n",
    "\n",
    "df_results[\"LLM_NAICS_Kodu\"] = llm_codes\n",
    "df_results[\"LLM_NAICS_Kategorisi\"] = llm_titles\n",
    "df_results[\"LLM_Aciklama\"] = llm_reasonings\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) KAYDET\n",
    "# =========================\n",
    "df_results.to_excel(OUT_PATH, index=False)\n",
    "print(f\"\\n Tamamlandı. Sonuçlar '{OUT_PATH}' dosyasına kaydedildi.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
